{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wLDQ6Kf8T_Aj"
      },
      "outputs": [],
      "source": [
        "# Import the dependencies\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from nltk.corpus import stopwords\n",
        "# Import CountVectorizer, TfidfVectorizer from sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "# Initialize the stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Import regex\n",
        "import re\n",
        "# Create a regex pattern to remove punctuation. \n",
        "pattern = r'[^a-zA-Z\\s ]'\n",
        "# Import the pipeline class from the transformers module. \n",
        "from transformers import pipeline\n",
        "# Import gradio \n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import the SentenceTransformer class and the utility function from the sentence_transformers library.\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "# Use the all-MiniLM-L6-v2 model.\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the CSV file into a DataFrame.\n",
        "king_james_bible_df = pd.read_csv('Resources/t_kjv.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the CSV file into a text format.\n",
        "filepath = \"Resources/t_kjv.csv\"\n",
        "with open(filepath) as f:\n",
        "    bible_text = f.read().replace('\\n',' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>v</th>\n",
              "      <th>t</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1001001</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>In the beginning God created the heaven and th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1001002</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>And the earth was without form, and void; and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1001003</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>And God said, Let there be light: and there wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1001004</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>And God saw the light, that it was good: and G...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001005</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>And God called the light Day, and the darkness...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  b  c  v                                                  t\n",
              "0  1001001  1  1  1  In the beginning God created the heaven and th...\n",
              "1  1001002  1  1  2  And the earth was without form, and void; and ...\n",
              "2  1001003  1  1  3  And God said, Let there be light: and there wa...\n",
              "3  1001004  1  1  4  And God saw the light, that it was good: and G...\n",
              "4  1001005  1  1  5  And God called the light Day, and the darkness..."
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print the first 5 lines of the data frame\n",
        "king_james_bible_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'id,b,c,v,t 1001001,1,1,1,In the beginning God created the heaven and the earth. 1001002,1,1,2,\"And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.\" 1001003,1,1,3,\"And God said, Let there be light: and there was light.\" 1001004,1,1,4,\"And God saw the light, that it was good: and God divided the light from the darkness.\" 1001005,1,1,5,\"And God called the light Day, and the darkness he called Night. And the evening and the morning were the first day.\" 1001006,1,1,6,\"And God said, Let there be a firmament in the midst of the waters, and let it divide the waters from the waters.\" 1001007,1,1,7,\"And God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament: and it was so.\" 1001008,1,1,8,And God called the firmament Heaven. And the evening and the morning were the second day. 1001009,1,1,9,\"And God said, Let the waters under the heaven b'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print the first 1000 items in the text\n",
        "bible_text[0:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Potentially relevant Code used in Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a list of sentences to tokenize.\n",
        "sentences = [\"I love my dog.\", \"I love my family.\", \"My dog is a lab.\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the BertTokenizer from the transformers package.\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the BertTokenizer on the pre-trained data.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i', 'am', 'learning', 'about', 'sub', '##word', 'token', '##ization', '.']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define an input text.\n",
        "text = \"I am learning about subword tokenization.\"\n",
        "\n",
        "# Tokenize the text into subwords.\n",
        "subwords = tokenizer.tokenize(text)\n",
        "subwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Lb0gz28aaHY_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\pipelines\\__init__.py:1081: UserWarning: \"translation\" task was used, instead of \"translation_XX_to_YY\", defaulting to \"translation_en_to_de\"\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Initialize the pipeline to translate using the t5-base model. \n",
        "translator = pipeline(\"translation\", model=\"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'translation_text': 'Ich feiere meinen Geburtstag.'}]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Ich feiere meinen Geburtstag.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a English text and translate it to German. \n",
        "english_text = \"I am celebrating my birthday.\"\n",
        "text = f\"translate English to German: {english_text}\"\n",
        "results = translator(text)\n",
        "# Display the translation JSON data. \n",
        "print(results)\n",
        "# Get the translated text.\n",
        "results[0]['translation_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AutoTokenizer and Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the Autotokenizer class from the transformers module. \n",
        "from transformers import AutoTokenizer\n",
        "# Create an instance of the Autotokenizer class using the t5-base model.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", max_length=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define text we want to translate.\n",
        "english_text = \"Hello, how are you today?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 13), dtype=int32, numpy=\n",
              "array([[13959,  1566,    12,  2379,    10,  8774,     6,   149,    33,\n",
              "           25,   469,    58,     1]])>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Retrieve the input IDs from the translation.\n",
        "input_ids = tokenizer(f\"translate English to French: {english_text}\", return_tensors=\"tf\").input_ids\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the TFAutoModelForSeq2SeqLM class from the transformers module. \n",
        "from transformers import TFAutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 14), dtype=int32, numpy=\n",
              "array([[    0, 21845,     6,  1670,   327,     3,  6738,    18,  3249,\n",
              "         7082,    31,  3464,    58,     1]])>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate the numerical outputs from the model. \n",
        "translation_model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-base\", max_length=100)\n",
        "output_ids = translation_model.generate(input_ids, max_new_tokens=100)\n",
        "output_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"<pad> Bonjour, comment vous êtes-vous aujourd'hui?</s>\""
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Decode the numerical outputs \n",
        "tokenizer.decode(output_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Bonjour, comment vous êtes-vous aujourd'hui?\""
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Retrieve the text from the special characters.\n",
        "tokenizer.decode(output_ids[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the text-generation parameter for the pipeline and EleutherAI/gpt-neo-1.3B model. \n",
        "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I like gardening because I like to help grow things. This blog is about my gardening experiences, thoughts, and ideas. If you ever run across any of my articles and ideas they might be useful to you. Also, do check out my “What’s Up” page for more gardening posts and updates.\n",
            "\n",
            "There are many different types of plants that you could grow. It’s your decision on what type of flowers you want to grow as well. There are also different types of gardeners because gardening can be a very rewarding job or you can spend your entire life growing only certain flowers.\n"
          ]
        }
      ],
      "source": [
        "# Give the model a prompt. \n",
        "prompt = \"I like gardening because\"\n",
        "# Pass the prompt to the generator\n",
        "results = generator(prompt, max_length=125, pad_token_id=50256)\n",
        "# Get the text based on the prompt. \n",
        "generated_text = results[0]['generated_text']\n",
        "# Print the generated text.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the text-generation parameter for the pipeline and EleutherAI/gpt-neo-125m model. \n",
        "small_generator = pipeline('text-generation', model='EleutherAI/gpt-neo-125m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My favorite animal is the cat because ive never seen a cat before. ive never seen a cat before. ive\n"
          ]
        }
      ],
      "source": [
        "# Give the model a prompt. \n",
        "prompt = \"My favorite animal is the cat because \"\n",
        "# Pass the prompt to the generator. Use `max_length=25`.\n",
        "new_results = small_generator(prompt, max_length=25, pad_token_id=50256)\n",
        "# Get the text based on the prompt. \n",
        "generated_text = new_results[0]['generated_text']\n",
        "# Print the generated text.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the pipeline class from the transformers module. \n",
        "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Source: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\n",
        "text = \"\"\"\n",
        "A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP)[1] and computer vision (CV).[2]\n",
        "\n",
        "Like recurrent neural networks (RNNs), transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs, transformers process the entire input all at once. The attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more parallelization than RNNs and therefore reduces training times.[1]\n",
        "\n",
        "Transformers were introduced in 2017 by a team at Google Brain[1] and are increasingly becoming the model of choice for NLP problems,[3] replacing RNN models such as long short-term memory (LSTM). The additional training parallelization allows training on larger datasets. This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks.[4][5]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a list of questions.\n",
        "questions = [\"When were transformers first introduced?\",\n",
        "             \"What are transformers better than?\",\n",
        "             \"What are applications of transformers?\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.9091500639915466, 'start': 864, 'end': 868, 'answer': '2017'}"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the output from one question.\n",
        "question = \"When were transformers first introduced?\"\n",
        "# Pass the first question and text to the question_answerer.\n",
        "result = question_answerer(question=question, context=text)\n",
        "# Show the results\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a function to generate the answers based on an input text.\n",
        "def question_answer(questions, text):\n",
        "    # Create a list to hold the data that will be added to the DataFrame.\n",
        "    data = []\n",
        "    # Use a for loop to iterate through the questions.\n",
        "    for question in questions:\n",
        "        # Pass the question and text to the initialized question_answerer. \n",
        "        result = question_answerer(question=question, context=text)\n",
        "        # Retrieve the question, answer, the score, the starting \n",
        "        # and ending of where the answer is located in the text.\n",
        "        data.append([question, result['answer'], result['score'], result['start'], result['end']])\n",
        "    # Create a DataFrame from the data with appropriate columns. \n",
        "    df = pd.DataFrame(data, columns=[\"Question\", \"Answer\", \"Score\", \"Starting Position\", \"Ending Position\"])\n",
        "    # Return the DataFrame\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Score</th>\n",
              "      <th>Starting Position</th>\n",
              "      <th>Ending Position</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When were transformers first introduced?</td>\n",
              "      <td>2017</td>\n",
              "      <td>0.909150</td>\n",
              "      <td>864</td>\n",
              "      <td>868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are transformers better than?</td>\n",
              "      <td>RNNs</td>\n",
              "      <td>0.575204</td>\n",
              "      <td>481</td>\n",
              "      <td>485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What are applications of transformers?</td>\n",
              "      <td>translation and text summarization</td>\n",
              "      <td>0.855785</td>\n",
              "      <td>429</td>\n",
              "      <td>463</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Question  \\\n",
              "0  When were transformers first introduced?   \n",
              "1        What are transformers better than?   \n",
              "2    What are applications of transformers?   \n",
              "\n",
              "                               Answer     Score  Starting Position  \\\n",
              "0                                2017  0.909150                864   \n",
              "1                                RNNs  0.575204                481   \n",
              "2  translation and text summarization  0.855785                429   \n",
              "\n",
              "   Ending Position  \n",
              "0              868  \n",
              "1              485  \n",
              "2              463  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Call the question_answer function with the questions and text. From Activity 21-2-5\n",
        "question_answer(questions, text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the pipeline class for summarization using the facebook/bart-large-cnn model.\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a variable to contain the text from (https://en.wikipedia.org/wiki/Deep_learning) to summarize.\n",
        "article =\"\"\"Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.[2] \n",
        "\n",
        "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]\n",
        "\n",
        "Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[6][7]\n",
        "\n",
        "The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation that is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'summary_text': 'Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep-learning architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design and medical image analysis.'}]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the most likely summary of the article using \"False\" for the `do_sample` parameter.\n",
        "most_likely_summary = summarizer(article, \n",
        "                     min_length=30, \n",
        "                     max_length=130, \n",
        "                     do_sample=False)\n",
        "\n",
        "# Display the summary\n",
        "most_likely_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep-learning architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design and medical image analysis.'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the summary text from the JSON output\n",
        "most_likely_summary[0][\"summary_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. It has been applied to fields including computer vision, speech recognition, natural language processing, machine translation and bioinformatics.'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a more diverse summary of the article using \"True\" for the `do_sample` parameter.\n",
        "diverse_summary = summarizer(article, \n",
        "                     min_length=30, \n",
        "                     max_length=130, \n",
        "                     do_sample=True)[0][\"summary_text\"]\n",
        "\n",
        "# Display the summary\n",
        "diverse_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Question and Answer with Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the pipeline to generate questions and answers using the distilbert-base-cased-distilled-squad model. \n",
        "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a function called `question_answer` that takes two parameters, the text to search and a question.\n",
        "# The function should return the question, answer, probability score, and the starting and ending index of the answer.\n",
        "def question_answer(text, question):\n",
        "    result = question_answerer(question=question, context=text)\n",
        "    return question, result['answer'], result['score'], result['start'], result['end']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7861\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the app with two Textbox components. \n",
        "# The first textbox will take the text to search the second will take the question.\n",
        "# The output should show the question, answer, probability score, and the starting and ending index of the answer.\n",
        "\n",
        "app = gr.Interface(\n",
        "    fn=question_answer,\n",
        "    inputs = [\n",
        "        gr.Textbox(label=\"Paste the text to search.\"), \n",
        "        gr.Textbox(label=\"Ask a question.\")],\n",
        "    outputs=gr.Textbox(lines=10, label=\"Answer to question, probability score, and location.\", show_copy_button=True))\n",
        "    \n",
        "# Launch the app.\n",
        "app.launch(show_error=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Summarizer Function with Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a summary function passing in the desired parameters. \n",
        "def summarize(article, max_output):\n",
        "    return f'{summarizer(article, max_length=max_output, min_length=30, do_sample=False)[0][\"summary_text\"]}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7862\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create an instance of the Gradio Interface application function with parameters. \n",
        "app = gr.Interface(fn=summarize, \n",
        "                   title=\"Text Summarizer using Transformers\",\n",
        "                   inputs=[\"text\", \"number\"], \n",
        "                   outputs=gr.Textbox(lines=20, label=\"Summarized Text Output\", show_copy_button=True))\n",
        "# Launch the app\n",
        "app.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
