{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "wLDQ6Kf8T_Aj"
      },
      "outputs": [],
      "source": [
        "# Import the dependencies\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from nltk.corpus import stopwords\n",
        "# Import CountVectorizer, TfidfVectorizer from sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "# Initialize the stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Import regex\n",
        "import re\n",
        "# Create a regex pattern to remove punctuation. \n",
        "pattern = r'[^a-zA-Z\\s ]'\n",
        "# Import the pipeline class from the transformers module. \n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the SentenceTransformer class and the utility function from the sentence_transformers library.\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "# Use the all-MiniLM-L6-v2 model.\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a list of sentences to tokenize.\n",
        "sentences = [\"I love my dog.\", \"I love my family.\", \"My dog is a lab.\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the BertTokenizer from the transformers package.\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the BertTokenizer on the pre-trained data.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i', 'am', 'learning', 'about', 'sub', '##word', 'token', '##ization', '.']"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define an input text.\n",
        "text = \"I am learning about subword tokenization.\"\n",
        "\n",
        "# Tokenize the text into subwords.\n",
        "subwords = tokenizer.tokenize(text)\n",
        "subwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Lb0gz28aaHY_"
      },
      "outputs": [],
      "source": [
        "# Initialize the pipeline to translate using the t5-base model. \n",
        "translator = pipeline(\"translation\", model=\"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'translation_text': 'Ich feiere meinen Geburtstag.'}]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Ich feiere meinen Geburtstag.'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a English text and translate it to German. \n",
        "english_text = \"I am celebrating my birthday.\"\n",
        "text = f\"translate English to German: {english_text}\"\n",
        "results = translator(text)\n",
        "# Display the translation JSON data. \n",
        "print(results)\n",
        "# Get the translated text.\n",
        "results[0]['translation_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AutoTokenizer and Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the Autotokenizer class from the transformers module. \n",
        "from transformers import AutoTokenizer\n",
        "# Create an instance of the Autotokenizer class using the t5-base model.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", max_length=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define text we want to translate.\n",
        "english_text = \"Hello, how are you today?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 13), dtype=int32, numpy=\n",
              "array([[13959,  1566,    12,  2379,    10,  8774,     6,   149,    33,\n",
              "           25,   469,    58,     1]])>"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Retrieve the input IDs from the translation.\n",
        "input_ids = tokenizer(f\"translate English to French: {english_text}\", return_tensors=\"tf\").input_ids\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the TFAutoModelForSeq2SeqLM class from the transformers module. \n",
        "from transformers import TFAutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 14), dtype=int32, numpy=\n",
              "array([[    0, 21845,     6,  1670,   327,     3,  6738,    18,  3249,\n",
              "         7082,    31,  3464,    58,     1]])>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate the numerical outputs from the model. \n",
        "translation_model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-base\", max_length=100)\n",
        "output_ids = translation_model.generate(input_ids, max_new_tokens=100)\n",
        "output_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"<pad> Bonjour, comment vous êtes-vous aujourd'hui?</s>\""
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Decode the numerical outputs \n",
        "tokenizer.decode(output_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Bonjour, comment vous êtes-vous aujourd'hui?\""
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Retrieve the text from the special characters.\n",
        "tokenizer.decode(output_ids[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the text-generation parameter for the pipeline and EleutherAI/gpt-neo-1.3B model. \n",
        "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I like gardening because no one has thought of gardening like me—or you. In the past, I have found it easier to write my life story than to simply write history. I find it easier to connect the dots between generations, and, while I am not necessarily a natural historian, I am interested in connecting events from the past, and in connecting generations when they have diverged. I find it easier to write about the past than the present because I can see that history is not always linear, sometimes it is more circular—which is, of course, part of what makes it fun to write about.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Give the model a prompt. \n",
        "prompt = \"I like gardening because\"\n",
        "# Pass the prompt to the generator\n",
        "results = generator(prompt, max_length=125, pad_token_id=50256)\n",
        "# Get the text based on the prompt. \n",
        "generated_text = results[0]['generated_text']\n",
        "# Print the generated text.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the text-generation parameter for the pipeline and EleutherAI/gpt-neo-125m model. \n",
        "small_generator = pipeline('text-generation', model='EleutherAI/gpt-neo-125m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My favorite animal is the cat because ive never seen a cat before. ive never seen a cat before. ive\n"
          ]
        }
      ],
      "source": [
        "# Give the model a prompt. \n",
        "prompt = \"My favorite animal is the cat because \"\n",
        "# Pass the prompt to the generator. Use `max_length=25`.\n",
        "new_results = small_generator(prompt, max_length=25, pad_token_id=50256)\n",
        "# Get the text based on the prompt. \n",
        "generated_text = new_results[0]['generated_text']\n",
        "# Print the generated text.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the pipeline class from the transformers module. \n",
        "from transformers import pipeline\n",
        "# Initialize the pipeline to generate questions and answers using the distilbert-base-cased-distilled-squad model. \n",
        "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Source: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\n",
        "text = \"\"\"\n",
        "A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP)[1] and computer vision (CV).[2]\n",
        "\n",
        "Like recurrent neural networks (RNNs), transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs, transformers process the entire input all at once. The attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more parallelization than RNNs and therefore reduces training times.[1]\n",
        "\n",
        "Transformers were introduced in 2017 by a team at Google Brain[1] and are increasingly becoming the model of choice for NLP problems,[3] replacing RNN models such as long short-term memory (LSTM). The additional training parallelization allows training on larger datasets. This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks.[4][5]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a list of questions.\n",
        "questions = [\"When were transformers first introduced?\",\n",
        "             \"What are transformers better than?\",\n",
        "             \"What are applications of transformers?\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.9091500639915466, 'start': 864, 'end': 868, 'answer': '2017'}"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the output from one question.\n",
        "question = \"When were transformers first introduced?\"\n",
        "# Pass the first question and text to the question_answerer.\n",
        "result = question_answerer(question=question, context=text)\n",
        "# Show the results\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a function to generate the answers based on an input text.\n",
        "def question_answer(questions, text):\n",
        "    # Create a list to hold the data that will be added to the DataFrame.\n",
        "    data = []\n",
        "    # Use a for loop to iterate through the questions.\n",
        "    for question in questions:\n",
        "        # Pass the question and text to the initialized question_answerer. \n",
        "        result = question_answerer(question=question, context=text)\n",
        "        # Retrieve the question, answer, the score, the starting \n",
        "        # and ending of where the answer is located in the text.\n",
        "        data.append([question, result['answer'], result['score'], result['start'], result['end']])\n",
        "    # Create a DataFrame from the data with appropriate columns. \n",
        "    df = pd.DataFrame(data, columns=[\"Question\", \"Answer\", \"Score\", \"Starting Position\", \"Ending Position\"])\n",
        "    # Return the DataFrame\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Score</th>\n",
              "      <th>Starting Position</th>\n",
              "      <th>Ending Position</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When were transformers first introduced?</td>\n",
              "      <td>2017</td>\n",
              "      <td>0.909150</td>\n",
              "      <td>864</td>\n",
              "      <td>868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are transformers better than?</td>\n",
              "      <td>RNNs</td>\n",
              "      <td>0.575204</td>\n",
              "      <td>481</td>\n",
              "      <td>485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What are applications of transformers?</td>\n",
              "      <td>translation and text summarization</td>\n",
              "      <td>0.855785</td>\n",
              "      <td>429</td>\n",
              "      <td>463</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Question  \\\n",
              "0  When were transformers first introduced?   \n",
              "1        What are transformers better than?   \n",
              "2    What are applications of transformers?   \n",
              "\n",
              "                               Answer     Score  Starting Position  \\\n",
              "0                                2017  0.909150                864   \n",
              "1                                RNNs  0.575204                481   \n",
              "2  translation and text summarization  0.855785                429   \n",
              "\n",
              "   Ending Position  \n",
              "0              868  \n",
              "1              485  \n",
              "2              463  "
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Call the question_answer function with the questions and text. From Activity 21-2-5\n",
        "question_answer(questions, text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
