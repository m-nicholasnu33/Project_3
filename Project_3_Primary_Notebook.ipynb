{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wLDQ6Kf8T_Aj"
      },
      "outputs": [
        {
          "ename": "PydanticSchemaGenerationError",
          "evalue": "Unable to generate pydantic-core schema for <class 'pandas.core.frame.DataFrame'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n\nIf you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\n\nFor further information visit https://errors.pydantic.dev/2.9/u/schema-for-unknown-type",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPydanticSchemaGenerationError\u001b[0m             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Import potential requirements to create a Retreival Augmented Generation (RAG) capability\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_stores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISSDocumentStore\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnodes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DensePassageRetriever, FARMReader\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExtractiveQAPipeline\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\haystack\\__init__.py:10\u001b[0m\n\u001b[0;32m      6\u001b[0m __version__: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(metadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfarm-haystack\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msilenceable_tqdm\u001b[39;00m  \u001b[38;5;66;03m# Needs to be imported first to wrap TQDM for all following modules\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document, Answer, Label, MultiLabel, Span, EvaluationResult, TableCell\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnodes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseComponent\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\haystack\\schema.py:42\u001b[0m\n\u001b[0;32m     36\u001b[0m FilterType \u001b[38;5;241m=\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], List[Any], \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m]]\n\u001b[0;32m     38\u001b[0m LABEL_DATETIME_FORMAT: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129;43m@dataclass\u001b[39;49m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mDocument\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mUnion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m]\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\dataclasses.py:264\u001b[0m, in \u001b[0;36mdataclass\u001b[1;34m(_cls, init, repr, eq, order, unsafe_hash, frozen, config, validate_on_init, kw_only, slots)\u001b[0m\n\u001b[0;32m    261\u001b[0m     _pydantic_dataclasses\u001b[38;5;241m.\u001b[39mcomplete_dataclass(\u001b[38;5;28mcls\u001b[39m, config_wrapper, raise_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, types_namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m create_dataclass \u001b[38;5;28;01mif\u001b[39;00m _cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mcreate_dataclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_cls\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\dataclasses.py:261\u001b[0m, in \u001b[0;36mdataclass.<locals>.create_dataclass\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m original_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_complete__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# `complete_dataclass` will set it to `True` if successful.\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m \u001b[43m_pydantic_dataclasses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete_dataclass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes_namespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_dataclasses.py:160\u001b[0m, in \u001b[0;36mcomplete_dataclass\u001b[1;34m(cls, config_wrapper, raise_errors, types_namespace)\u001b[0m\n\u001b[0;32m    151\u001b[0m         schema \u001b[38;5;241m=\u001b[39m get_core_schema(\n\u001b[0;32m    152\u001b[0m             \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m    153\u001b[0m             CallbackGetCoreSchemaHandler(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m             ),\n\u001b[0;32m    158\u001b[0m         )\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[43mgen_schema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_dunder_get_core_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PydanticUndefinedAnnotation \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_errors:\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:655\u001b[0m, in \u001b[0;36mGenerateSchema.generate_schema\u001b[1;34m(self, obj, from_dunder_get_core_schema)\u001b[0m\n\u001b[0;32m    652\u001b[0m         schema \u001b[38;5;241m=\u001b[39m from_property\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m _extract_get_pydantic_json_schema(obj, schema)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:929\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_inner\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n\u001b[1;32m--> 929\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1025\u001b[0m, in \u001b[0;36mGenerateSchema.match_type\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zoneinfo_schema()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _typing_extra\u001b[38;5;241m.\u001b[39mis_dataclass(obj):\n\u001b[1;32m-> 1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataclass_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1027\u001b[0m origin \u001b[38;5;241m=\u001b[39m get_origin(obj)\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1822\u001b[0m, in \u001b[0;36mGenerateSchema._dataclass_schema\u001b[1;34m(self, dataclass, origin)\u001b[0m\n\u001b[0;32m   1819\u001b[0m decorators \u001b[38;5;241m=\u001b[39m dataclass\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__pydantic_decorators__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m DecoratorInfos\u001b[38;5;241m.\u001b[39mbuild(dataclass)\n\u001b[0;32m   1820\u001b[0m \u001b[38;5;66;03m# Move kw_only=False args to the start of the list, as this is how vanilla dataclasses work.\u001b[39;00m\n\u001b[0;32m   1821\u001b[0m \u001b[38;5;66;03m# Note that when kw_only is missing or None, it is treated as equivalent to kw_only=True\u001b[39;00m\n\u001b[1;32m-> 1822\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1823\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_dc_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkw_only\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1825\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1826\u001b[0m has_post_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(dataclass, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__post_init__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1827\u001b[0m has_slots \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(dataclass, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__slots__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1823\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1819\u001b[0m decorators \u001b[38;5;241m=\u001b[39m dataclass\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__pydantic_decorators__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m DecoratorInfos\u001b[38;5;241m.\u001b[39mbuild(dataclass)\n\u001b[0;32m   1820\u001b[0m \u001b[38;5;66;03m# Move kw_only=False args to the start of the list, as this is how vanilla dataclasses work.\u001b[39;00m\n\u001b[0;32m   1821\u001b[0m \u001b[38;5;66;03m# Note that when kw_only is missing or None, it is treated as equivalent to kw_only=True\u001b[39;00m\n\u001b[0;32m   1822\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m-> 1823\u001b[0m     (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_dc_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mitems()),\n\u001b[0;32m   1824\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m a: a\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkw_only\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1825\u001b[0m )\n\u001b[0;32m   1826\u001b[0m has_post_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(dataclass, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__post_init__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1827\u001b[0m has_slots \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(dataclass, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__slots__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1132\u001b[0m, in \u001b[0;36mGenerateSchema._generate_dc_field_schema\u001b[1;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_dc_field_schema\u001b[39m(\n\u001b[0;32m   1126\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1127\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1128\u001b[0m     field_info: FieldInfo,\n\u001b[0;32m   1129\u001b[0m     decorators: DecoratorInfos,\n\u001b[0;32m   1130\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mDataclassField:\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prepare a DataclassField to represent the parameter/field, of a dataclass.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1132\u001b[0m     common_field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_common_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdataclass_field(\n\u001b[0;32m   1134\u001b[0m         name,\n\u001b[0;32m   1135\u001b[0m         common_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1143\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   1144\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1308\u001b[0m, in \u001b[0;36mGenerateSchema._common_field_schema\u001b[1;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[0;32m   1304\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_annotations(\n\u001b[0;32m   1305\u001b[0m             source_type, annotations \u001b[38;5;241m+\u001b[39m validators_from_decorators, transform_inner_schema\u001b[38;5;241m=\u001b[39mset_discriminator\n\u001b[0;32m   1306\u001b[0m         )\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1308\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_annotations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m            \u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m            \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidators_from_decorators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;66;03m# This V1 compatibility shim should eventually be removed\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;66;03m# push down any `each_item=True` validators\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;66;03m# note that this won't work for any Annotated types that get wrapped by a function validator\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;66;03m# but that's okay because that didn't exist in V1\u001b[39;00m\n\u001b[0;32m   1317\u001b[0m this_field_validators \u001b[38;5;241m=\u001b[39m filter_field_decorator_info_by_field(decorators\u001b[38;5;241m.\u001b[39mvalidators\u001b[38;5;241m.\u001b[39mvalues(), name)\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2107\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations\u001b[1;34m(self, source_type, annotations, transform_inner_schema)\u001b[0m\n\u001b[0;32m   2102\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   2103\u001b[0m     get_inner_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_wrapped_inner_schema(\n\u001b[0;32m   2104\u001b[0m         get_inner_schema, annotation, pydantic_js_annotation_functions\n\u001b[0;32m   2105\u001b[0m     )\n\u001b[1;32m-> 2107\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mget_inner_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pydantic_js_annotation_functions:\n\u001b[0;32m   2109\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m CoreMetadataHandler(schema)\u001b[38;5;241m.\u001b[39mmetadata\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py:83\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[1;34m(self, source_type)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[1;32m---> 83\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2088\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations.<locals>.inner_handler\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2086\u001b[0m from_property \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_from_property(obj, source_type)\n\u001b[0;32m   2087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_property \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2088\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2090\u001b[0m     schema \u001b[38;5;241m=\u001b[39m from_property\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:929\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_inner\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n\u001b[1;32m--> 929\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1029\u001b[0m, in \u001b[0;36mGenerateSchema.match_type\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m   1027\u001b[0m origin \u001b[38;5;241m=\u001b[39m get_origin(obj)\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_generic_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1031\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_prepare_pydantic_annotations_for_known_type(obj, ())\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1058\u001b[0m, in \u001b[0;36mGenerateSchema._match_generic_type\u001b[1;34m(self, obj, origin)\u001b[0m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m from_property\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _typing_extra\u001b[38;5;241m.\u001b[39morigin_is_union(origin):\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_union_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m origin \u001b[38;5;129;01min\u001b[39;00m TUPLE_TYPES:\n\u001b[0;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tuple_schema(obj)\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1378\u001b[0m, in \u001b[0;36mGenerateSchema._union_schema\u001b[1;34m(self, union_type)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         nullable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         choices\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(choices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1381\u001b[0m     s \u001b[38;5;241m=\u001b[39m choices[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:655\u001b[0m, in \u001b[0;36mGenerateSchema.generate_schema\u001b[1;34m(self, obj, from_dunder_get_core_schema)\u001b[0m\n\u001b[0;32m    652\u001b[0m         schema \u001b[38;5;241m=\u001b[39m from_property\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m _extract_get_pydantic_json_schema(obj, schema)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:929\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_inner\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n\u001b[1;32m--> 929\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1038\u001b[0m, in \u001b[0;36mGenerateSchema.match_type\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arbitrary_types:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arbitrary_type_schema(obj)\n\u001b[1;32m-> 1038\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unknown_type_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:558\u001b[0m, in \u001b[0;36mGenerateSchema._unknown_type_schema\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unknown_type_schema\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CoreSchema:\n\u001b[1;32m--> 558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticSchemaGenerationError(\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to generate pydantic-core schema for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    560\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSet `arbitrary_types_allowed=True` in the model_config to ignore this error\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or implement `__get_pydantic_core_schema__` on your type to fully support it.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you got this error by calling handler(<some type>) within\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m `__get_pydantic_core_schema__` then you likely need to call\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m `handler.generate_schema(<some type>)` since we do not call\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    566\u001b[0m     )\n",
            "\u001b[1;31mPydanticSchemaGenerationError\u001b[0m: Unable to generate pydantic-core schema for <class 'pandas.core.frame.DataFrame'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n\nIf you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\n\nFor further information visit https://errors.pydantic.dev/2.9/u/schema-for-unknown-type"
          ]
        }
      ],
      "source": [
        "# Import the dependencies\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from nltk.corpus import stopwords\n",
        "# Import CountVectorizer, TfidfVectorizer from sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "# Initialize the stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Import regex\n",
        "import re\n",
        "# Create a regex pattern to remove punctuation. \n",
        "pattern = r'[^a-zA-Z\\s ]'\n",
        "# Import the pipeline class from the transformers module. \n",
        "from transformers import pipeline\n",
        "# Import gradio \n",
        "import gradio as gr\n",
        "# Import potential requirements to create a Retreival Augmented Generation (RAG) capability\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from haystack.document_stores import FAISSDocumentStore\n",
        "from haystack.nodes import DensePassageRetriever, FARMReader\n",
        "from haystack.pipelines import ExtractiveQAPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import the SentenceTransformer class and the utility function from the sentence_transformers library.\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "# Use the all-MiniLM-L6-v2 model.\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the CSV file into a DataFrame.\n",
        "king_james_bible_df = pd.read_csv('Resources/t_kjv.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the CSV file into a text format.\n",
        "filepath = \"Resources/t_kjv.csv\"\n",
        "with open(filepath) as f:\n",
        "    bible_text = f.read().replace('\\n',' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>v</th>\n",
              "      <th>t</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1001001</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>In the beginning God created the heaven and th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1001002</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>And the earth was without form, and void; and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1001003</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>And God said, Let there be light: and there wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1001004</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>And God saw the light, that it was good: and G...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001005</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>And God called the light Day, and the darkness...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  b  c  v                                                  t\n",
              "0  1001001  1  1  1  In the beginning God created the heaven and th...\n",
              "1  1001002  1  1  2  And the earth was without form, and void; and ...\n",
              "2  1001003  1  1  3  And God said, Let there be light: and there wa...\n",
              "3  1001004  1  1  4  And God saw the light, that it was good: and G...\n",
              "4  1001005  1  1  5  And God called the light Day, and the darkness..."
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print the first 5 lines of the data frame\n",
        "king_james_bible_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'id,b,c,v,t 1001001,1,1,1,In the beginning God created the heaven and the earth. 1001002,1,1,2,\"And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.\" 1001003,1,1,3,\"And God said, Let there be light: and there was light.\" 1001004,1,1,4,\"And God saw the light, that it was good: and God divided the light from the darkness.\" 1001005,1,1,5,\"And God called the light Day, and the darkness he called Night. And the evening and the morning were the first day.\" 1001006,1,1,6,\"And God said, Let there be a firmament in the midst of the waters, and let it divide the waters from the waters.\" 1001007,1,1,7,\"And God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament: and it was so.\" 1001008,1,1,8,And God called the firmament Heaven. And the evening and the morning were the second day. 1001009,1,1,9,\"And God said, Let the waters under the heaven b'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print the first 1000 items in the text\n",
        "bible_text[0:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Potentially relevant Code used in Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a list of sentences to tokenize.\n",
        "sentences = [\"I love my dog.\", \"I love my family.\", \"My dog is a lab.\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the BertTokenizer from the transformers package.\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the BertTokenizer on the pre-trained data.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i', 'am', 'learning', 'about', 'sub', '##word', 'token', '##ization', '.']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define an input text.\n",
        "text = \"I am learning about subword tokenization.\"\n",
        "\n",
        "# Tokenize the text into subwords.\n",
        "subwords = tokenizer.tokenize(text)\n",
        "subwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Lb0gz28aaHY_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\pipelines\\__init__.py:1086: UserWarning: \"translation\" task was used, instead of \"translation_XX_to_YY\", defaulting to \"translation_en_to_de\"\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Initialize the pipeline to translate using the t5-base model. \n",
        "translator = pipeline(\"translation\", model=\"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'translation_text': 'Ich feiere meinen Geburtstag.'}]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Ich feiere meinen Geburtstag.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a English text and translate it to German. \n",
        "english_text = \"I am celebrating my birthday.\"\n",
        "text = f\"translate English to German: {english_text}\"\n",
        "results = translator(text)\n",
        "# Display the translation JSON data. \n",
        "print(results)\n",
        "# Get the translated text.\n",
        "results[0]['translation_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AutoTokenizer and Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the Autotokenizer class from the transformers module. \n",
        "from transformers import AutoTokenizer\n",
        "# Create an instance of the Autotokenizer class using the t5-base model.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", max_length=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define text we want to translate.\n",
        "english_text = \"Hello, how are you today?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 13), dtype=int32, numpy=\n",
              "array([[13959,  1566,    12,  2379,    10,  8774,     6,   149,    33,\n",
              "           25,   469,    58,     1]])>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Retrieve the input IDs from the translation.\n",
        "input_ids = tokenizer(f\"translate English to French: {english_text}\", return_tensors=\"tf\").input_ids\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the TFAutoModelForSeq2SeqLM class from the transformers module. \n",
        "from transformers import TFAutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 14), dtype=int32, numpy=\n",
              "array([[    0, 21845,     6,  1670,   327,     3,  6738,    18,  3249,\n",
              "         7082,    31,  3464,    58,     1]])>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate the numerical outputs from the model. \n",
        "translation_model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-base\", max_length=100)\n",
        "output_ids = translation_model.generate(input_ids, max_new_tokens=100)\n",
        "output_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"<pad> Bonjour, comment vous êtes-vous aujourd'hui?</s>\""
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Decode the numerical outputs \n",
        "tokenizer.decode(output_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Bonjour, comment vous êtes-vous aujourd'hui?\""
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Retrieve the text from the special characters.\n",
        "tokenizer.decode(output_ids[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the text-generation parameter for the pipeline and EleutherAI/gpt-neo-1.3B model. \n",
        "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I like gardening because of the outdoors. My home is a garden, we are surrounded, the view is a must. I do not know what to do when it is not my time to be outdoors enjoying the outdoors and being exposed to nature, such as when I was little. My question is, do I have an obligation to plant my yard with flowers for the beauty of it? I live in an area that is not known for wild grasses. I have never heard anyone complain that wild grass was not present at their home. I live in a state known for the large acreage of grass that grow. Do I\n"
          ]
        }
      ],
      "source": [
        "# Give the model a prompt. \n",
        "prompt = \"I like gardening because\"\n",
        "# Pass the prompt to the generator\n",
        "results = generator(prompt, max_length=125, pad_token_id=50256)\n",
        "# Get the text based on the prompt. \n",
        "generated_text = results[0]['generated_text']\n",
        "# Print the generated text.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the text-generation parameter for the pipeline and EleutherAI/gpt-neo-125m model. \n",
        "small_generator = pipeline('text-generation', model='EleutherAI/gpt-neo-125m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My favorite animal is the cat because ive never seen a cat before. ive never seen a cat before. ive\n"
          ]
        }
      ],
      "source": [
        "# Give the model a prompt. \n",
        "prompt = \"My favorite animal is the cat because \"\n",
        "# Pass the prompt to the generator. Use `max_length=25`.\n",
        "new_results = small_generator(prompt, max_length=25, pad_token_id=50256)\n",
        "# Get the text based on the prompt. \n",
        "generated_text = new_results[0]['generated_text']\n",
        "# Print the generated text.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the pipeline class from the transformers module. \n",
        "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Source: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\n",
        "text = \"\"\"\n",
        "A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP)[1] and computer vision (CV).[2]\n",
        "\n",
        "Like recurrent neural networks (RNNs), transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs, transformers process the entire input all at once. The attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more parallelization than RNNs and therefore reduces training times.[1]\n",
        "\n",
        "Transformers were introduced in 2017 by a team at Google Brain[1] and are increasingly becoming the model of choice for NLP problems,[3] replacing RNN models such as long short-term memory (LSTM). The additional training parallelization allows training on larger datasets. This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks.[4][5]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a list of questions.\n",
        "questions = [\"When were transformers first introduced?\",\n",
        "             \"What are transformers better than?\",\n",
        "             \"What are applications of transformers?\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.9091500639915466, 'start': 864, 'end': 868, 'answer': '2017'}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the output from one question.\n",
        "question = \"When were transformers first introduced?\"\n",
        "# Pass the first question and text to the question_answerer.\n",
        "result = question_answerer(question=question, context=text)\n",
        "# Show the results\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a function to generate the answers based on an input text.\n",
        "def question_answer(questions, text):\n",
        "    # Create a list to hold the data that will be added to the DataFrame.\n",
        "    data = []\n",
        "    # Use a for loop to iterate through the questions.\n",
        "    for question in questions:\n",
        "        # Pass the question and text to the initialized question_answerer. \n",
        "        result = question_answerer(question=question, context=text)\n",
        "        # Retrieve the question, answer, the score, the starting \n",
        "        # and ending of where the answer is located in the text.\n",
        "        data.append([question, result['answer'], result['score'], result['start'], result['end']])\n",
        "    # Create a DataFrame from the data with appropriate columns. \n",
        "    df = pd.DataFrame(data, columns=[\"Question\", \"Answer\", \"Score\", \"Starting Position\", \"Ending Position\"])\n",
        "    # Return the DataFrame\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Score</th>\n",
              "      <th>Starting Position</th>\n",
              "      <th>Ending Position</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When were transformers first introduced?</td>\n",
              "      <td>2017</td>\n",
              "      <td>0.909150</td>\n",
              "      <td>864</td>\n",
              "      <td>868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are transformers better than?</td>\n",
              "      <td>RNNs</td>\n",
              "      <td>0.575204</td>\n",
              "      <td>481</td>\n",
              "      <td>485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What are applications of transformers?</td>\n",
              "      <td>translation and text summarization</td>\n",
              "      <td>0.855785</td>\n",
              "      <td>429</td>\n",
              "      <td>463</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Question  \\\n",
              "0  When were transformers first introduced?   \n",
              "1        What are transformers better than?   \n",
              "2    What are applications of transformers?   \n",
              "\n",
              "                               Answer     Score  Starting Position  \\\n",
              "0                                2017  0.909150                864   \n",
              "1                                RNNs  0.575204                481   \n",
              "2  translation and text summarization  0.855785                429   \n",
              "\n",
              "   Ending Position  \n",
              "0              868  \n",
              "1              485  \n",
              "2              463  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Call the question_answer function with the questions and text. From Activity 21-2-5\n",
        "question_answer(questions, text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the pipeline class for summarization using the facebook/bart-large-cnn model.\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a variable to contain the text from (https://en.wikipedia.org/wiki/Deep_learning) to summarize.\n",
        "article =\"\"\"Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.[2] \n",
        "\n",
        "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]\n",
        "\n",
        "Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[6][7]\n",
        "\n",
        "The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation that is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'summary_text': 'Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep-learning architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design and medical image analysis.'}]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the most likely summary of the article using \"False\" for the `do_sample` parameter.\n",
        "most_likely_summary = summarizer(article, \n",
        "                     min_length=30, \n",
        "                     max_length=130, \n",
        "                     do_sample=False)\n",
        "\n",
        "# Display the summary\n",
        "most_likely_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep-learning architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design and medical image analysis.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the summary text from the JSON output\n",
        "most_likely_summary[0][\"summary_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep-learning architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a more diverse summary of the article using \"True\" for the `do_sample` parameter.\n",
        "diverse_summary = summarizer(article, \n",
        "                     min_length=30, \n",
        "                     max_length=130, \n",
        "                     do_sample=True)[0][\"summary_text\"]\n",
        "\n",
        "# Display the summary\n",
        "diverse_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Question and Answer with Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the pipeline to generate questions and answers using the distilbert-base-cased-distilled-squad model. \n",
        "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a function called `question_answer` that takes two parameters, the text to search and a question.\n",
        "# The function should return the question, answer, probability score, and the starting and ending index of the answer.\n",
        "def question_answer(text, question):\n",
        "    result = question_answerer(question=question, context=text)\n",
        "    return question, result['answer'], result['score'], result['start'], result['end']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the app with two Textbox components. \n",
        "# The first textbox will take the text to search the second will take the question.\n",
        "# The output should show the question, answer, probability score, and the starting and ending index of the answer.\n",
        "\n",
        "app = gr.Interface(\n",
        "    fn=question_answer,\n",
        "    inputs = [\n",
        "        gr.Textbox(label=\"Paste the text to search.\"), \n",
        "        gr.Textbox(label=\"Ask a question.\")],\n",
        "    outputs=gr.Textbox(lines=10, label=\"Answer to question, probability score, and location.\", show_copy_button=True))\n",
        "    \n",
        "# Launch the app.\n",
        "app.launch(show_error=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Summarizer Function with Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a summary function passing in the desired parameters. \n",
        "def summarize(article, max_output):\n",
        "    return f'{summarizer(article, max_length=max_output, min_length=30, do_sample=False)[0][\"summary_text\"]}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7861\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create an instance of the Gradio Interface application function with parameters. \n",
        "app = gr.Interface(fn=summarize, \n",
        "                   title=\"Text Summarizer using Transformers\",\n",
        "                   inputs=[\"text\", \"number\"], \n",
        "                   outputs=gr.Textbox(lines=20, label=\"Summarized Text Output\", show_copy_button=True))\n",
        "# Launch the app\n",
        "app.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating an RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d39459c31b84a6eac6bcde9af53ba9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mnich\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mnich\\.cache\\huggingface\\hub\\models--facebook--bart-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "# Load your pre-trained Language Model, like BART or T5\n",
        "model_name = \"facebook/bart-large\"  # or \"google/t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'FAISSDocumentStore' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Set up a Retrieval System, FAISS, to create a document index for retrieval\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m document_store \u001b[38;5;241m=\u001b[39m \u001b[43mFAISSDocumentStore\u001b[49m(faiss_index_factory_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m retriever \u001b[38;5;241m=\u001b[39m DensePassageRetriever(\n\u001b[0;32m      4\u001b[0m     document_store\u001b[38;5;241m=\u001b[39mdocument_store,\n\u001b[0;32m      5\u001b[0m     query_embedding_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/dpr-question_encoder-single-nq-base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     passage_embedding_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/dpr-ctx_encoder-single-nq-base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n",
            "\u001b[1;31mNameError\u001b[0m: name 'FAISSDocumentStore' is not defined"
          ]
        }
      ],
      "source": [
        "# Set up a Retrieval System, FAISS, to create a document index for retrieval\n",
        "document_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\")\n",
        "retriever = DensePassageRetriever(\n",
        "    document_store=document_store,\n",
        "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
        "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# May not be able to use FAISS.  It requires Haystack-Farm, which is not compatible with Pydantic >2.0, but Gradio requires Pydantic >2.7.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'document_store' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Add a document to document store\u001b[39;00m\n\u001b[0;32m      2\u001b[0m docs \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour document text here\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument 1\u001b[39m\u001b[38;5;124m\"\u001b[39m}}]  \n\u001b[1;32m----> 3\u001b[0m \u001b[43mdocument_store\u001b[49m\u001b[38;5;241m.\u001b[39mwrite_documents(docs)\n\u001b[0;32m      4\u001b[0m document_store\u001b[38;5;241m.\u001b[39mupdate_embeddings(retriever)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'document_store' is not defined"
          ]
        }
      ],
      "source": [
        "# Add a document to document store\n",
        "docs = [{\"content\": \"Your document text here\", \"meta\": {\"source\": \"Document 1\"}}]  \n",
        "document_store.write_documents(docs)\n",
        "document_store.update_embeddings(retriever)  # Update the document embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'FARMReader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Setting up the retrieval and reading pipeline\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43mFARMReader\u001b[49m(model_name_or_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepset/roberta-base-squad2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m ExtractiveQAPipeline(reader, retriever)\n\u001b[0;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour query here\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'FARMReader' is not defined"
          ]
        }
      ],
      "source": [
        "# Setting up the retrieval and reading pipeline\n",
        "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\")\n",
        "pipeline = ExtractiveQAPipeline(reader, retriever)\n",
        "\n",
        "query = \"Your query here\"\n",
        "result = pipeline.run(query=query, top_k_retriever=10, top_k_reader=5)\n",
        "\n",
        "# See retrieved documents and answers\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'documents'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[46], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Integrate with the Model for augmented generation\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#  # Retrieve documents and combine them into a context\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m retrieved_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Generate text using the context\u001b[39;00m\n\u001b[0;32m      7\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour input question here: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m retrieved_context\n",
            "\u001b[1;31mKeyError\u001b[0m: 'documents'"
          ]
        }
      ],
      "source": [
        "# Integrate with the Model for augmented generation\n",
        "# \n",
        "#  # Retrieve documents and combine them into a context\n",
        "retrieved_context = \" \".join([doc['content'] for doc in result['documents']])\n",
        "\n",
        "# Generate text using the context\n",
        "input_text = \"Your input question here: \" + retrieved_context\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "outputs = model.generate(**inputs, max_length=150)\n",
        "\n",
        "# Decode the output\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
